{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "PY0101EN-4-2-WriteFile.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLt2Ps3k0up7"
      },
      "source": [
        "<a href=\"https://cognitiveclass.ai/\">\n",
        "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/PY0101EN/Ad/CCLog.png\" width=\"200\" align=\"center\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43rIWbQI0up-"
      },
      "source": [
        "<h1>Write and Save Files in Python</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyCBqDWd0uqA"
      },
      "source": [
        "<p><strong>Welcome!</strong> This notebook will teach you about write the text to file in the Python Programming Language. By the end of this lab, you'll know how to write to file and copy the file.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_hgPfRe0uqC"
      },
      "source": [
        "<h2>Table of Contents</h2>\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "    <ul>\n",
        "        <li><a href=\"write\">Writing Files</a></li>\n",
        "        <li><a href=\"copy\">Copy a File</a></li>\n",
        "    </ul>\n",
        "    <p>\n",
        "        Estimated time needed: <strong>15 min</strong>\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZJjO76a0uqD"
      },
      "source": [
        "<h2 id=\"write\">Writing Files</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFDcJI4g0uqF"
      },
      "source": [
        " We can open a file object using the method <code>write()</code> to save the text file to a list. To write the mode, argument must be set to write <b>w</b>. Let’s write a file <b>Example2.txt</b> with the line: <b>“This is line A”</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U3RCqa90uqG"
      },
      "source": [
        "# Write line to file\n",
        "\n",
        "with open('/resources/data/Example2.txt', 'w') as writefile:\n",
        "    writefile.write(\"This is line A\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpcJPaUy0uqP"
      },
      "source": [
        " We can read the file to see if it worked:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_ExPCuj0uqQ"
      },
      "source": [
        "# Read file\n",
        "\n",
        "with open('/resources/data/Example2.txt', 'r') as testwritefile:\n",
        "    print(testwritefile.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPX4_QDP0uqW"
      },
      "source": [
        "We can write multiple lines:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h3zbOfP0uqX"
      },
      "source": [
        "# Write lines to file\n",
        "\n",
        "with open('/resources/data/Example2.txt', 'w') as writefile:\n",
        "    writefile.write(\"This is line A\\n\")\n",
        "    writefile.write(\"This is line B\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_UJ-8eF0uqe"
      },
      "source": [
        "The method <code>.write()</code> works similar to the method <code>.readline()</code>, except instead of reading a new line it writes a new line. The process is illustrated in the figure , the different colour coding of the grid represents a new line added to the file after each method call."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv_Vsntd0uqf"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/PY0101EN/Chapter%204/Images/WriteLine.png\" width=\"500\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "974YTupv0uqh"
      },
      "source": [
        "You can check the file to see if your results are correct "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fsf2L9zK0uqi"
      },
      "source": [
        "# Check whether write to file\n",
        "\n",
        "with open('/resources/data/Example2.txt', 'r') as testwritefile:\n",
        "    print(testwritefile.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_gfSZAd0uqn"
      },
      "source": [
        " By setting the mode argument to append **a**  you can append a new line as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFFgFOl00uqo"
      },
      "source": [
        "# Write a new line to text file\n",
        "\n",
        "with open('/resources/data/Example2.txt', 'a') as testwritefile:\n",
        "    testwritefile.write(\"This is line C\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N23uTexm0uqx"
      },
      "source": [
        " You can verify the file has changed by running the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JETFtv8p0uqy"
      },
      "source": [
        "# Verify if the new line is in the text file\n",
        "\n",
        "with open('/resources/data/Example2.txt', 'r') as testwritefile:\n",
        "    print(testwritefile.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar3kQmbt0uq3"
      },
      "source": [
        " We write a list to a <b>.txt</b> file  as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8An-Bco0uq4"
      },
      "source": [
        "# Sample list of text\n",
        "\n",
        "Lines = [\"This is line A\\n\", \"This is line B\\n\", \"This is line C\\n\"]\n",
        "Lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIC-7duI0urD"
      },
      "source": [
        "# Write the strings in the list to text file\n",
        "\n",
        "with open('Example2.txt', 'w') as writefile:\n",
        "    for line in Lines:\n",
        "        print(line)\n",
        "        writefile.write(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suNT40OX0urJ"
      },
      "source": [
        " We can verify the file is written by reading it and printing out the values:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eBFo9FN0urL"
      },
      "source": [
        "# Verify if writing to file is successfully executed\n",
        "\n",
        "with open('Example2.txt', 'r') as testwritefile:\n",
        "    print(testwritefile.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWezcpUX0urS"
      },
      "source": [
        "We can again append to the file by changing the second parameter to <b>a</b>. This adds the code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY8JZ4rf0urT"
      },
      "source": [
        "# Append the line to the file\n",
        "\n",
        "with open('Example2.txt', 'a') as testwritefile:\n",
        "    testwritefile.write(\"This is line D\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H69mXXBN0urY"
      },
      "source": [
        "We can see the results of appending the file: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS0mzIs40urZ"
      },
      "source": [
        "# Verify if the appending is successfully executed\n",
        "\n",
        "with open('Example2.txt', 'r') as testwritefile:\n",
        "    print(testwritefile.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apQo5B860urf"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VepnvnDs0urg"
      },
      "source": [
        "<h2 id=\"copy\">Copy a File</h2> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9Cc3FKF0urh"
      },
      "source": [
        "Let's copy the file <b>Example2.txt</b> to the file <b>Example3.txt</b>:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO8CimOD0uri"
      },
      "source": [
        "# Copy file to another\n",
        "\n",
        "with open('Example2.txt','r') as readfile:\n",
        "    with open('Example3.txt','w') as writefile:\n",
        "          for line in readfile:\n",
        "                writefile.write(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS366siV0urm"
      },
      "source": [
        "We can read the file to see if everything works:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r7RRA680urn"
      },
      "source": [
        "# Verify if the copy is successfully executed\n",
        "\n",
        "with open('Example3.txt','r') as testwritefile:\n",
        "    print(testwritefile.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dEoW7qP0urr"
      },
      "source": [
        " After reading files, we can also write data into files and save them in different file formats like **.txt, .csv, .xls (for excel files) etc**. Let's take a look at some examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f7Z-fVa0urs"
      },
      "source": [
        "Now go to the directory to ensure the <b>.txt</b> file exists and contains the summary data that we wrote."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-8XLrR70urt"
      },
      "source": [
        "<hr>\n",
        "<h2>The last exercise!</h2>\n",
        "<p>Congratulations, you have completed your first lesson and hands-on lab in Python. However, there is one more thing you need to do. The Data Science community encourages sharing work. The best way to share and showcase your work is to share it on GitHub. By sharing your notebook on GitHub you are not only building your reputation with fellow data scientists, but you can also show it off when applying for a job. Even though this was your first piece of work, it is never too early to start building good habits. So, please read and follow <a href=\"https://cognitiveclass.ai/blog/data-scientists-stand-out-by-sharing-your-notebooks/\" target=\"_blank\">this article</a> to learn how to share your work.\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYk5NtO30urv"
      },
      "source": [
        "<h3>About the Authors:</h3>  \n",
        "<p><a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\" target=\"_blank\">Joseph Santarcangelo</a> is a Data Scientist at IBM, and holds a PhD in Electrical Engineering. His research focused on using Machine Learning, Signal Processing, and Computer Vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrRD81Rt0urw"
      },
      "source": [
        "<p>Other contributors: <a href=\"https://www.linkedin.com/in/jiahui-mavis-zhou-a4537814a\">Mavis Zhou</a>, <a href=\"https://github.com/computationalcore\" target=\"_blank\">Vin Busquet</a>, <a href=\"https://github.com/raphtrajano\" target=\"_blank\">Raph Trajano</a></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FORRJMgV0urx"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pqwWMiz0ury"
      },
      "source": [
        "<p>Copyright &copy; 2018 IBM Developer Skills Network. This notebook and its source code are released under the terms of the <a href=\"https://cognitiveclass.ai/mit-license/\">MIT License</a>.</p>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "\n",
        "text = open('dataset.txt', 'r', errors='ignore').read()\n",
        "words = re.findall('[a-z]+|[0-9]+', text, flags=re.IGNORECASE)\n",
        "\n",
        "# cleaned text\n",
        "text = ' '.join(words).lower()\n",
        "words = text.split(' ')\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "\n",
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "\n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
        "\n",
        "ids = ids_from_chars(chars)\n",
        "\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "\n",
        "\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)\n",
        "\n",
        "tf.exp(example_batch_mean_loss).numpy()\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "EPOCHS = 30\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states\n",
        "\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['life'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)\n",
        "\n",
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp_FDrDsgfda",
        "outputId": "65182665-6057-45bd-db8b-51773c6e20aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 38) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"my_model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     multiple                  9728      \n",
            "                                                                 \n",
            " gru_2 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_2 (Dense)             multiple                  38950     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,986,982\n",
            "Trainable params: 3,986,982\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Prediction shape:  (64, 100, 38)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(3.6374743, shape=(), dtype=float32)\n",
            "Epoch 1/30\n",
            "148/148 [==============================] - 799s 5s/step - loss: 2.4970\n",
            "Epoch 2/30\n",
            " 17/148 [==>...........................] - ETA: 11:39 - loss: 2.0283"
          ]
        }
      ]
    }
  ]
}